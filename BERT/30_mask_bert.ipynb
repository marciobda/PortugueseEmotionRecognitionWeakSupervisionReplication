{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google colab, once\n",
    "# !pip install imblearn\n",
    "# !wget \"https://raw.githubusercontent.com/marciobda/PortugueseEmotionRecognitionWeakSupervision/refs/heads/main/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:57:51.616690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731437871.632231  181119 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731437871.636874  181119 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 15:57:51.655174: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#general purpose\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "#data processing\n",
    "import re, string\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#transformers\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFBertModel\n",
    "\n",
    "#keras\n",
    "import tensorflow as tf\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#seed for reproducibility\n",
    "SEED = 42\n",
    "MODEL_NAME = 'bert_30_mask'\n",
    "MASK_PROABILITY = .3\n",
    "\n",
    "#set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['Admiração', 'Diversão', 'Raiva', 'Aborrecimento', 'Aprovação', 'Confusão', 'Curiosidade', 'Desejo', 'Decepção', 'Nojo', 'Vergonha', 'Entusiasmo', 'Medo', 'Gratidão', 'Luto', 'Alegria', 'Amor', 'Nervosismo', 'Otimismo', 'Orgulho', 'Alívio', 'Remorso', 'Tristeza', 'Surpresa', 'Saudade', 'Inveja', 'Compaixão', 'Desaprovação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(y, y_pred, title):\n",
    "    fig, ax =plt.subplots(figsize=(40,40))\n",
    "    labels=emotions\n",
    "    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":20})\n",
    "    plt.title(title, fontsize=30)\n",
    "    ax.xaxis.set_ticklabels(labels, fontsize=20) \n",
    "    ax.yaxis.set_ticklabels(labels, fontsize=20)\n",
    "    ax.set_ylabel('Test', fontsize=22)\n",
    "    ax.set_xlabel('Predicted', fontsize=22)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1407463675905167370</td>\n",
       "      <td>Quem é esse rei ja virei fã, a lenda so falou ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1407447387472334852</td>\n",
       "      <td>Finalmente dei uma faxina bonita na casa! A se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1407508991303892993</td>\n",
       "      <td>A incrível arte de flertar com várias pessoas ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1407473645983764480</td>\n",
       "      <td>Babo muito em Thaylane cr, q preta maravilhosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1407916016278163456</td>\n",
       "      <td>Bolo da minha tia ficou muito feio mas oq tinh...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                              tweet  \\\n",
       "0  1407463675905167370  Quem é esse rei ja virei fã, a lenda so falou ...   \n",
       "1  1407447387472334852  Finalmente dei uma faxina bonita na casa! A se...   \n",
       "2  1407508991303892993  A incrível arte de flertar com várias pessoas ...   \n",
       "3  1407473645983764480     Babo muito em Thaylane cr, q preta maravilhosa   \n",
       "4  1407916016278163456  Bolo da minha tia ficou muito feio mas oq tinh...   \n",
       "\n",
       "  categoria  \n",
       "0         1  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4        26  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./../train.csv', sep='\\t', quoting=3 , engine='python')\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1407769371955634180</td>\n",
       "      <td>nossa sério eu daria tudo p saber das fofocas ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1407860353598427138</td>\n",
       "      <td>Sem palavras p agradecer tudo o que tem aconte...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1407855425782005771</td>\n",
       "      <td>tava respondendo tudo isso na minha cabeça e m...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1407740654428561414</td>\n",
       "      <td>eu achei que era possível terminar sem pegar n...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1407865324989521921</td>\n",
       "      <td>Sim mas n o amor romântico</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                              tweet  \\\n",
       "0  1407769371955634180  nossa sério eu daria tudo p saber das fofocas ...   \n",
       "1  1407860353598427138  Sem palavras p agradecer tudo o que tem aconte...   \n",
       "2  1407855425782005771  tava respondendo tudo isso na minha cabeça e m...   \n",
       "3  1407740654428561414  eu achei que era possível terminar sem pegar n...   \n",
       "4  1407865324989521921                         Sim mas n o amor romântico   \n",
       "\n",
       "  categoria  \n",
       "0         6  \n",
       "1        13  \n",
       "2        12  \n",
       "3         5  \n",
       "4        16  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('./../test.csv', sep='\\t', quoting=3 , engine='python')\n",
    "df_test = df_test.dropna()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10612 entries, 0 to 10728\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweet_id   10612 non-null  object\n",
      " 1   tweet      10612 non-null  object\n",
      " 2   categoria  10612 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 331.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10525 entries, 0 to 10728\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweet_id   10525 non-null  object\n",
      " 1   tweet      10525 non-null  object\n",
      " 2   categoria  10525 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 328.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "df.drop_duplicates(subset='tweet', inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2657 entries, 0 to 2681\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweet_id   2657 non-null   int64 \n",
      " 1   tweet      2657 non-null   object\n",
      " 2   categoria  2657 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 83.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2648 entries, 0 to 2681\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweet_id   2648 non-null   int64 \n",
      " 1   tweet      2648 non-null   object\n",
      " 2   categoria  2648 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 82.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()\n",
    "df_test.drop_duplicates(subset='tweet', inplace=True)\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['tweet', 'categoria']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations, links, mentions and \\r\\n new line characers\n",
    "def strip_all_entities(text):\n",
    "    text = text.replace('\\r','').replace('\\n',' ').lower()\n",
    "    text = re.sub(r'(?:\\@|https?\\://)\\S+', '',text)\n",
    "    text = re.sub(r'[^\\x00-\\x7f]','', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n",
    "\n",
    "    banned_list = string.punctuation\n",
    "\n",
    "    table = str.maketrans('','',banned_list)\n",
    "    text = text.translate(table)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Filter special characters such as & and $ present in some words\n",
    "def filter_chars(text):\n",
    "    sent = []\n",
    "    for word in text.split(' '):\n",
    "        if ('$' in word) | ('&' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "    \n",
    "    return ' '.join(sent)\n",
    "\n",
    "# Remove multiple spaces\n",
    "def remove_mult_spaces(text):\n",
    "    return re.sub(r'\\s\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = []\n",
    "\n",
    "for text in df.tweet:\n",
    "    new_texts.append(remove_mult_spaces(filter_chars(strip_all_entities(text))))\n",
    "\n",
    "df['clean_tweet'] = df.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts_test = []\n",
    "\n",
    "for text in df_test.tweet:\n",
    "    new_texts_test.append(remove_mult_spaces(filter_chars(strip_all_entities(text))))\n",
    "\n",
    "df_test['clean_tweet'] = df_test.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    nossa sério eu daria tudo p saber das fofocas ...\n",
       "1    Sem palavras p agradecer tudo o que tem aconte...\n",
       "2    tava respondendo tudo isso na minha cabeça e m...\n",
       "3    eu achei que era possível terminar sem pegar n...\n",
       "4                           Sim mas n o amor romântico\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_tweet'].head()\n",
    "df_test['clean_tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lens = []\n",
    "\n",
    "for text in df['clean_tweet'].values:\n",
    "    tokens = tokenizer.encode(text, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))\n",
    "\n",
    "max_len = np.max(token_lens)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lens = []\n",
    "\n",
    "for text in df_test['clean_tweet'].values:\n",
    "    tokens = tokenizer.encode(text, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))\n",
    "\n",
    "max_len = np.max(token_lens)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion category analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoria\n",
       "5        1189\n",
       "1         874\n",
       "7         798\n",
       "2         740\n",
       "0         545\n",
       "13        541\n",
       "22        494\n",
       "12        469\n",
       "16        419\n",
       "23        339\n",
       "26        305\n",
       "18        303\n",
       "20        300\n",
       "17        287\n",
       "15        273\n",
       "9         270\n",
       "6         263\n",
       "8         252\n",
       "3         248\n",
       "24        244\n",
       "21        243\n",
       "11        232\n",
       "19        216\n",
       "25        177\n",
       "27        172\n",
       "4         148\n",
       "10        147\n",
       "14         28\n",
       "11,15       5\n",
       "12,13       1\n",
       "18,19       1\n",
       "11,17       1\n",
       "16,17       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['categoria'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoria\n",
       "5        320\n",
       "7        222\n",
       "1        204\n",
       "2        193\n",
       "0        134\n",
       "12       126\n",
       "13       117\n",
       "22       109\n",
       "16       108\n",
       "26        88\n",
       "23        81\n",
       "18        76\n",
       "17        73\n",
       "20        73\n",
       "6         69\n",
       "9         67\n",
       "15        66\n",
       "11        61\n",
       "19        60\n",
       "24        60\n",
       "21        58\n",
       "8         57\n",
       "3         50\n",
       "10        45\n",
       "4         44\n",
       "27        43\n",
       "25        36\n",
       "14         7\n",
       "20,21      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['categoria'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['categoria'] = df['categoria'].apply(ast.literal_eval)\n",
    "df = df.explode('categoria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['categoria'] = df_test['categoria'].apply(ast.literal_eval)\n",
    "df_test = df_test.explode('categoria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoria\n",
       "5     1189\n",
       "1      874\n",
       "7      798\n",
       "2      740\n",
       "0      545\n",
       "13     542\n",
       "22     494\n",
       "12     470\n",
       "16     420\n",
       "23     339\n",
       "26     305\n",
       "18     304\n",
       "20     300\n",
       "17     289\n",
       "15     278\n",
       "9      270\n",
       "6      263\n",
       "8      252\n",
       "3      248\n",
       "24     244\n",
       "21     243\n",
       "11     238\n",
       "19     217\n",
       "25     177\n",
       "27     172\n",
       "4      148\n",
       "10     147\n",
       "14      28\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['categoria'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoria\n",
       "5     320\n",
       "7     222\n",
       "1     204\n",
       "2     193\n",
       "0     134\n",
       "12    126\n",
       "13    117\n",
       "22    109\n",
       "16    108\n",
       "26     88\n",
       "23     81\n",
       "18     76\n",
       "20     74\n",
       "17     73\n",
       "6      69\n",
       "9      67\n",
       "15     66\n",
       "11     61\n",
       "19     60\n",
       "24     60\n",
       "21     59\n",
       "8      57\n",
       "3      50\n",
       "10     45\n",
       "4      44\n",
       "27     43\n",
       "25     36\n",
       "14      7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['categoria'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Validation - Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10534, 3) 10534 1589 1060\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_RATIO = 0.15\n",
    "TEST_RATIO = 0.10\n",
    "\n",
    "X_train =  df['clean_tweet']\n",
    "X_test =  df_test['clean_tweet']\n",
    "y_train = df['categoria'].astype(str)\n",
    "y_test = df_test['categoria'].astype(str)\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test ,test_size= TEST_RATIO/(TEST_RATIO + VALIDATION_RATIO), random_state=SEED)\n",
    "\n",
    "print(df.shape, X_train.size, X_valid.size, X_test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_le = y_train.copy()\n",
    "y_valid_le = y_valid.copy()\n",
    "y_test_le = y_test.copy()\n",
    "\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "y_train = ohe.fit_transform(np.array(y_train).reshape(-1,1)).toarray()\n",
    "y_valid = ohe.fit_transform(np.array(y_valid).reshape(-1,1)).toarray()\n",
    "y_test = ohe.fit_transform(np.array(y_test).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Emotion analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_items = pd.read_csv('./../lexical_items.csv')\n",
    "lexical_items_array = np.array(lexical_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "MASK_TOKEN = tokenizer.convert_ids_to_tokens(tokenizer.mask_token_id)\n",
    "\n",
    "def masked_input(text):\n",
    "    aggregated = []\n",
    "    for word in text.split(' '):\n",
    "        if np.isin(word, lexical_items_array):\n",
    "            aggregated.append(MASK_TOKEN)\n",
    "        else:\n",
    "            aggregated.append(word)\n",
    "    sentence = ' '.join(aggregated)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def tokenize(data, max_len=MAX_LEN, mask_probability=0):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # Convert to list if it's a DataFrame/Series\n",
    "    if hasattr(data, 'values'):\n",
    "        data = data.values.tolist()\n",
    "\n",
    "    for text in data:\n",
    "\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            masked_input(text) if np.random.rand() < mask_probability else text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',  # Added for consistency\n",
    "            truncation=True,       # Added for safety\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_attention_masks = tokenize(X_train, MAX_LEN, MASK_PROABILITY)\n",
    "val_input_ids, val_attention_masks = tokenize(X_valid, MAX_LEN)\n",
    "test_input_ids, test_attention_masks = tokenize(X_test, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731438046.000916  181119 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9536 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:0c:00.0, compute capability: 8.6\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_model, max_len=MAX_LEN):\n",
    "\n",
    "    ## params ##\n",
    "    opt = tf.keras.optimizers.AdamW(learning_rate=1e-5)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "    input_ids = tf.keras.Input(shape=(max_len), dtype='int32')\n",
    "    attention_maks = tf.keras.Input(shape=(max_len), dtype='int32')\n",
    "\n",
    "    embeddings = bert_model([input_ids, attention_maks])[1]\n",
    "\n",
    "    output = tf.keras.layers.Dense(28, activation=\"softmax\")(embeddings)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_maks], outputs=output)\n",
    "    \n",
    "    model.compile(opt, loss=loss, metrics=accuracy)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1094822   ['input_1[0][0]',             \n",
      " )                           ngAndCrossAttentions(last_   40         'input_2[0][0]']             \n",
      "                             hidden_state=(None, 128, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 28)                   21532     ['tf_bert_model[0][1]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109503772 (417.72 MB)\n",
      "Trainable params: 109503772 (417.72 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(bert_model, MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731438069.400110  182486 service.cc:148] XLA service 0x7ff0809d83f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731438069.400155  182486 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2024-11-12 16:01:09.406967: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1731438069.417722  182486 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1731438069.475990  182486 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 111s 255ms/step - loss: 3.1203 - categorical_accuracy: 0.1408 - val_loss: 2.8107 - val_categorical_accuracy: 0.2580\n",
      "Epoch 2/4\n",
      "330/330 [==============================] - 82s 248ms/step - loss: 2.4669 - categorical_accuracy: 0.3574 - val_loss: 1.6670 - val_categorical_accuracy: 0.6104\n",
      "Epoch 3/4\n",
      "330/330 [==============================] - 82s 250ms/step - loss: 1.7715 - categorical_accuracy: 0.5618 - val_loss: 1.2732 - val_categorical_accuracy: 0.6916\n",
      "Epoch 4/4\n",
      "330/330 [==============================] - 82s 249ms/step - loss: 1.4755 - categorical_accuracy: 0.6243 - val_loss: 1.1871 - val_categorical_accuracy: 0.7124\n"
     ]
    }
   ],
   "source": [
    "history_bert = model.fit(\n",
    "    [train_input_ids, train_attention_masks],\n",
    "    y_train,\n",
    "    validation_data=([val_input_ids, val_attention_masks], y_valid),\n",
    "    epochs=4,\n",
    "    batch_size=32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 5s 96ms/step\n"
     ]
    }
   ],
   "source": [
    "result_bert = model.predict([test_input_ids,test_attention_masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bert =  np.zeros_like(result_bert)\n",
    "y_pred_bert[np.arange(len(y_pred_bert)), result_bert.argmax(1)] = 1\n",
    "\n",
    "conf_matrix(y_test.argmax(1), y_pred_bert.argmax(1),f'{MODEL_NAME} Sentiment Analysis\\nConfusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tClassification Report for bert_30_mask BERT:\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    Admiração       0.82      0.61      0.70        51\n",
      "     Diversão       0.46      0.69      0.55        74\n",
      "        Raiva       0.94      0.73      0.82        22\n",
      "Aborrecimento       0.94      0.71      0.81        21\n",
      "    Aprovação       0.66      0.71      0.68        56\n",
      "     Confusão       0.56      0.76      0.64        45\n",
      "  Curiosidade       0.00      0.00      0.00         4\n",
      "       Desejo       0.76      0.68      0.72        28\n",
      "     Decepção       0.67      0.63      0.65        38\n",
      "         Nojo       0.68      0.47      0.56        32\n",
      "     Vergonha       0.80      0.82      0.81        39\n",
      "   Entusiasmo       0.88      0.62      0.73        24\n",
      "         Medo       0.80      0.87      0.83        69\n",
      "     Gratidão       0.71      0.76      0.73        29\n",
      "         Luto       0.80      0.84      0.82        19\n",
      "      Alegria       0.68      0.74      0.71        43\n",
      "         Amor       0.86      0.76      0.81        41\n",
      "   Nervosismo       0.81      0.85      0.83        20\n",
      "     Otimismo       0.83      0.94      0.88        16\n",
      "      Orgulho       0.49      0.65      0.56        37\n",
      "       Alívio       0.62      0.67      0.64        12\n",
      "      Remorso       0.85      0.94      0.89        18\n",
      "     Tristeza       0.40      0.08      0.14        24\n",
      "     Surpresa       0.82      0.84      0.83       134\n",
      "      Saudade       0.92      0.48      0.63        23\n",
      "       Inveja       0.75      0.71      0.73        84\n",
      "    Compaixão       0.84      0.62      0.71        26\n",
      " Desaprovação       0.76      0.81      0.78        31\n",
      "\n",
      "    micro avg       0.72      0.72      0.72      1060\n",
      "    macro avg       0.72      0.68      0.69      1060\n",
      " weighted avg       0.73      0.72      0.71      1060\n",
      "  samples avg       0.72      0.72      0.72      1060\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marciobda/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(f'\\tClassification Report for {MODEL_NAME}:\\n\\n',classification_report(y_test, y_pred_bert, target_names=emotions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# model_save_path = f'{MODEL_NAME}_model'\n",
    "# os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# model.save(model_save_path)\n",
    "# tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text, tokenizer, max_len=128):\n",
    "\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "    return encoded_text['input_ids'], encoded_text['attention_mask']\n",
    "\n",
    "def predict_category(text, model, tokenizer, label_encoder):\n",
    "\n",
    "    input_ids, attention_mask = prepare_text(text, tokenizer)\n",
    "\n",
    "    predictions = model.predict([input_ids, attention_mask])\n",
    "\n",
    "    # Create a zero array with proper dimensions\n",
    "    one_hot = np.zeros(28)  # 28 is the number of categories\n",
    "    # Set 1 at the predicted class index\n",
    "    one_hot[predictions.argmax(axis=-1)[0]] = 1\n",
    "    # Reshape to match OneHotEncoder's expected format\n",
    "    one_hot = one_hot.reshape(1, -1)\n",
    "    \n",
    "    # Convert back to original category\n",
    "    predicted_label = ohe.inverse_transform(one_hot)[0]\n",
    "\n",
    "    return emotions[int(predicted_label[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_category' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEu estou empolgado com o trabalho\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m predicted_category \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_category\u001b[49m(text, model, tokenizer, ohe)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted categoy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_category\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_category' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"Eu estou empolgado com o trabalho\"\n",
    "predicted_category = predict_category(text, model, tokenizer, ohe)\n",
    "\n",
    "print(f\"Predicted categoy: {predicted_category}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
