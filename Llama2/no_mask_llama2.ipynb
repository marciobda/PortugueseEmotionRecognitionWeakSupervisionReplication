{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google colab, once\n",
    "# !pip install imblearn\n",
    "# !wget \"https://raw.githubusercontent.com/marciobda/PortugueseEmotionRecognitionWeakSupervision/refs/heads/main/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 00:11:26.789875: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731726686.801724  104409 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731726686.804993  104409 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-16 00:11:26.820795: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#general purpose\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import ast\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, Optional, List\n",
    "\n",
    "#data processing\n",
    "import re, string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#transformers\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "MODEL_NAME = 'llama2_no_mask'\n",
    "\n",
    "#set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['Admiração', 'Diversão', 'Raiva', 'Aborrecimento', 'Aprovação', 'Confusão', 'Curiosidade', 'Desejo', 'Decepção', 'Nojo', 'Vergonha', 'Entusiasmo', 'Medo', 'Gratidão', 'Luto', 'Alegria', 'Amor', 'Nervosismo', 'Otimismo', 'Orgulho', 'Alívio', 'Remorso', 'Tristeza', 'Surpresa', 'Saudade', 'Inveja', 'Compaixão', 'Desaprovação']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(y, y_pred, title):\n",
    "    fig, ax =plt.subplots(figsize=(40,40))\n",
    "    labels=emotions\n",
    "    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":20})\n",
    "    plt.title(title, fontsize=30)\n",
    "    ax.xaxis.set_ticklabels(labels, fontsize=20) \n",
    "    ax.yaxis.set_ticklabels(labels, fontsize=20)\n",
    "    ax.set_ylabel('Test', fontsize=22)\n",
    "    ax.set_xlabel('Predicted', fontsize=22)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1407769371955634180</td>\n",
       "      <td>nossa sério eu daria tudo p saber das fofocas ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1407860353598427138</td>\n",
       "      <td>Sem palavras p agradecer tudo o que tem aconte...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1407855425782005771</td>\n",
       "      <td>tava respondendo tudo isso na minha cabeça e m...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1407740654428561414</td>\n",
       "      <td>eu achei que era possível terminar sem pegar n...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1407865324989521921</td>\n",
       "      <td>Sim mas n o amor romântico</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                              tweet  \\\n",
       "0  1407769371955634180  nossa sério eu daria tudo p saber das fofocas ...   \n",
       "1  1407860353598427138  Sem palavras p agradecer tudo o que tem aconte...   \n",
       "2  1407855425782005771  tava respondendo tudo isso na minha cabeça e m...   \n",
       "3  1407740654428561414  eu achei que era possível terminar sem pegar n...   \n",
       "4  1407865324989521921                         Sim mas n o amor romântico   \n",
       "\n",
       "  categoria  \n",
       "0         6  \n",
       "1        13  \n",
       "2        12  \n",
       "3         5  \n",
       "4        16  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('./../test.csv', sep='\\t', quoting=3 , engine='python')\n",
    "df_test = df_test.dropna()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2657 entries, 0 to 2681\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweet_id   2657 non-null   int64 \n",
      " 1   tweet      2657 non-null   object\n",
      " 2   categoria  2657 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 83.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2648 entries, 0 to 2681\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweet_id   2648 non-null   int64 \n",
      " 1   tweet      2648 non-null   object\n",
      " 2   categoria  2648 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 82.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()\n",
    "df_test.drop_duplicates(subset='tweet', inplace=True)\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations, links, mentions and \\r\\n new line characers\n",
    "def strip_all_entities(text):\n",
    "    text = text.replace('\\r','').replace('\\n',' ').lower()\n",
    "    text = re.sub(r'(?:\\@|https?\\://)\\S+', '',text)\n",
    "    text = re.sub(r'[^\\x00-\\x7f]','', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n",
    "\n",
    "    banned_list = string.punctuation\n",
    "\n",
    "    table = str.maketrans('','',banned_list)\n",
    "    text = text.translate(table)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Filter special characters such as & and $ present in some words\n",
    "def filter_chars(text):\n",
    "    sent = []\n",
    "    for word in text.split(' '):\n",
    "        if ('$' in word) | ('&' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "    \n",
    "    return ' '.join(sent)\n",
    "\n",
    "# Remove multiple spaces\n",
    "def remove_mult_spaces(text):\n",
    "    return re.sub(r'\\s\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts_test = []\n",
    "\n",
    "for text in df_test.tweet:\n",
    "    new_texts_test.append(remove_mult_spaces(filter_chars(strip_all_entities(text))))\n",
    "\n",
    "df_test['clean_tweet'] = df_test.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    nossa sério eu daria tudo p saber das fofocas ...\n",
       "1    Sem palavras p agradecer tudo o que tem aconte...\n",
       "2    tava respondendo tudo isso na minha cabeça e m...\n",
       "3    eu achei que era possível terminar sem pegar n...\n",
       "4                           Sim mas n o amor romântico\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['clean_tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion category analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoria\n",
       "5        320\n",
       "7        222\n",
       "1        204\n",
       "2        193\n",
       "0        134\n",
       "12       126\n",
       "13       117\n",
       "22       109\n",
       "16       108\n",
       "26        88\n",
       "23        81\n",
       "18        76\n",
       "17        73\n",
       "20        73\n",
       "6         69\n",
       "9         67\n",
       "15        66\n",
       "11        61\n",
       "19        60\n",
       "24        60\n",
       "21        58\n",
       "8         57\n",
       "3         50\n",
       "10        45\n",
       "4         44\n",
       "27        43\n",
       "25        36\n",
       "14         7\n",
       "20,21      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['categoria'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['categoria'] = df_test['categoria'].apply(ast.literal_eval)\n",
    "df_test = df_test.explode('categoria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoria\n",
       "5     320\n",
       "7     222\n",
       "1     204\n",
       "2     193\n",
       "0     134\n",
       "12    126\n",
       "13    117\n",
       "22    109\n",
       "16    108\n",
       "26     88\n",
       "23     81\n",
       "18     76\n",
       "20     74\n",
       "17     73\n",
       "6      69\n",
       "9      67\n",
       "15     66\n",
       "11     61\n",
       "19     60\n",
       "24     60\n",
       "21     59\n",
       "8      57\n",
       "3      50\n",
       "10     45\n",
       "4      44\n",
       "27     43\n",
       "25     36\n",
       "14      7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['categoria'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Validation - Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llamma modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaTorchInferece:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        load_in_8bit: bool = False,\n",
    "        torch_dtype: torch.dtype = torch.float16\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.logger = self.__setup_logger()\n",
    "\n",
    "        self.logger.info(f\"Loading model {model_name} on {device}\")\n",
    "        self.logger.info(f\"Using dtype: {torch_dtype}\")\n",
    "\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        model_kwargs = {\n",
    "            \"device_map\": \"auto\" if device == \"cuda\" else None,\n",
    "            \"torch_dtype\": torch_dtype,\n",
    "        }\n",
    "\n",
    "        if load_in_8bit and device == \"cuda\":\n",
    "            self.logger.info(\"Loading model in 8bit precision\") \n",
    "            model_kwargs[\"load_in_8bit\"] = True\n",
    "\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            **model_kwargs,\n",
    "            local_files_only = True\n",
    "        )\n",
    "\n",
    "        if device == \"cpu\":\n",
    "            self.model = self.model.to(device)\n",
    "\n",
    "        self.logger.info(\"Model loaded successfully\")\n",
    "\n",
    "    def __setup_logger(self) -> logging.Logger:\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        return logging.getLogger(__name__)\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate_response(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.95,\n",
    "        top_k: int = 50,\n",
    "        num_return_sequences: int = 1,\n",
    "        do_sample: bool = True,\n",
    "        stop_sequences: Optional[list[str]] = None,\n",
    "    ) -> Dict:\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        input_length = inputs.input_ids.shape[1]\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        generated_text = self.tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\n",
    "\n",
    "        if stop_sequences:\n",
    "            for stop_sequence in stop_sequences:\n",
    "                if stop_sequence in generated_text:\n",
    "                    generated_text = generated_text[:generated_text.index(stop_sequence)]\n",
    "        \n",
    "        generation_time = time.time() - start_time\n",
    "\n",
    "        memory_usage = None\n",
    "        if self.device == \"cuda\":\n",
    "            memory_usage = {\n",
    "                \"allocated\": torch.cuda.memory_allocated() / 1024**2,\n",
    "                \"cached\": torch.cuda.memory_reserved() / 1024**2,\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"response\": generated_text,\n",
    "            \"generation_time\": f\"{generation_time:.2f} seconds\",\n",
    "            \"memory_usage\": memory_usage,\n",
    "        }\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        return {\n",
    "        \"model_type\": self.model.config.model_type,\n",
    "        \"vocab_size\": self.model.config.vocab_size,\n",
    "        \"hidden_size\": self.model.config.hidden_size,\n",
    "        \"num_attention_heads\": self.model.config.num_attention_heads,\n",
    "        \"num_hidden_layers\": self.model.config.num_hidden_layers,\n",
    "        \"device\": self.device,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading model meta-llama/Llama-2-7b-chat-hf on cuda\n",
      "INFO:__main__:Using dtype: torch.float16\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d9cc1a11ec467198856680248bd2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "INFO:__main__:Model loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model information:\n",
      "{'model_type': 'llama', 'vocab_size': 32000, 'hidden_size': 4096, 'num_attention_heads': 32, 'num_hidden_layers': 32, 'device': 'cuda'}\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "llama = LlamaTorchInferece(\n",
    "    model_name=MODEL_NAME,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"Model information:\")\n",
    "print(llama.get_model_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading model meta-llama/Llama-2-7b-chat-hf on cuda\n",
      "INFO:__main__:Using dtype: torch.float16\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297bce3421a4472b81f98372571dcdc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "INFO:__main__:Model loaded successfully\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model information:\n",
      "{'model_type': 'llama', 'vocab_size': 32000, 'hidden_size': 4096, 'num_attention_heads': 32, 'num_hidden_layers': 32, 'device': 'cuda'}\n",
      "\n",
      "Generated response: 🇫🇷 The capital of France is Paris! 😊 Would you like to know more about Paris or France in general?\n",
      "Generation Time: 24.44 seconds\n",
      "GPU Memory Usage: 9122.71 MB allocated\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Human: What is the capital of France?\n",
    "Assistant: \"\"\"\n",
    "\n",
    "result = llama.generate_response(\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    stop_sequences=[\"Human:\", \"Assistant:\"]\n",
    "\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated response:\", result[\"response\"])\n",
    "print(\"Generation Time:\", result['generation_time'])\n",
    "if result['memory_usage']:\n",
    "    print(f\"GPU Memory Usage: {result['memory_usage']['allocated']:.2f} MB allocated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated response: 😊 I'm just an AI, I don't have a physical location or a specific language, but I can help you with any questions or tasks you may have! How can I assist you today? 😊\n",
      "Generation Time: 122.39 seconds\n",
      "GPU Memory Usage: 258.14 MB allocated\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Você fala português?\n",
    "Assistant: \"\"\"\n",
    "\n",
    "result = llama.generate_response(\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        stop_sequences=[\"Human:\", \"Assistant:\"]\n",
    "\n",
    "    )\n",
    "\n",
    "print(\"\\nGenerated response:\", result[\"response\"])\n",
    "print(\"Generation Time:\", result['generation_time'])\n",
    "if result['memory_usage']:\n",
    "    print(f\"GPU Memory Usage: {result['memory_usage']['allocated']:.2f} MB allocated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated response: Of course, I can help you with that! In the given sentence, the most proeminent emotion is Alívio (Relief).\n",
      "Generation Time: 69.06 seconds\n",
      "GPU Memory Usage: 258.15 MB allocated\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Considere o conjunto de emoções: Admiração, Diversão, Raiva, Aborrecimento, Aprovação, Confusão, Curiosidade, Desejo, Decepção, Nojo, Vergonha, Entusiasmo, Medo, Gratidão, Luto, Alegria, Amor, Nervosismo, Otimismo, Orgulho, Alívio, Remorso, Tristeza, Surpresa, Saudade, Inveja, Compaixão, Desaprovação. Na frase 'Finalmente dei uma faxina bonita na casa! A sensação de alívio é maravilhosa' qual das emocões você consegue identificar? Responda apenas a emoção mais proeminente.\n",
    "Assistant: \"\"\"\n",
    "\n",
    "result = llama.generate_response(\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        stop_sequences=[\"Human:\", \"Assistant:\"]\n",
    "\n",
    "    )\n",
    "\n",
    "print(\"\\nGenerated response:\", result[\"response\"])\n",
    "print(\"Generation Time:\", result['generation_time'])\n",
    "if result['memory_usage']:\n",
    "    print(f\"GPU Memory Usage: {result['memory_usage']['allocated']:.2f} MB allocated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf_matrix(y_test.argmax(1), y_pred_bert.argmax(1),f'{MODEL_NAME} Sentiment Analysis\\nConfusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'\\tClassification Report for {MODEL_NAME}:\\n\\n',classification_report(y_test, y_pred_bert, target_names=emotions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Eu estou empolgado com o trabalho\"\n",
    "# predicted_category = predict_category(text, model, tokenizer, ohe)\n",
    "\n",
    "# print(f\"Predicted categoy: {predicted_category}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
